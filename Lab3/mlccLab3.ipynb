{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLCC - Laboratory 3 - Dimensionality reduction and feature selection\n",
    "In this laboratory we will address the problem of data analysis with a reference to a classification problem. \n",
    "Follow the instructions below.\n",
    "\n",
    "Import all the functions from the file \"lab2ImpFunction.py\" by: <br>\n",
    "`from lab3ImpFunction import *` <br>\n",
    "Also import pyplot for plotting: <br>\n",
    "`import matplotlib.pyplot as plt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Warm up - data generation\n",
    "\n",
    "* **1.A** Generate a training and a test set of D-dimensional ponts (N points for each class), with `N=100, D=30`. Only two of these dimensions will be meaningful, the remaining will be noise.\n",
    "\n",
    "    `n = 100`\n",
    "    \n",
    "    `dim = 30`\n",
    "\n",
    "    `Xtr, Ytr = MixGauss(means=[[1,-1],[1,1]],sigmas=[[0.7], [0.7]],n=n)\n",
    "`\n",
    "\n",
    "    `Xts, Yts = MixGauss(means=[[1,-1],[1,1]],sigmas=[[0.7], [0.7]],n=n)\n",
    "`\n",
    "\n",
    "    `Ytr = 2*np.mod(Ytr, 2)-1`\n",
    "\n",
    "    `Yts = 2*np.mod(Yts, 2)-1`\n",
    "    \n",
    "    \n",
    "* **1.B** You may want to plot the relevant variables of the data\n",
    "\n",
    "     `plt.scatter(Xtr[:,0], Xtr[:,1], s=30, c=np.squeeze(Ytr), alpha=0.5)`\n",
    "     \n",
    "     `plt.title('Plot data', fontsize=14, color='red')`\n",
    "     \n",
    "     `plt.show()`\n",
    "     \n",
    "     \n",
    "* **1.C** The remaining variables will be generated as gaussian noise\n",
    "\n",
    "    `sigma_noise = 0.01`\n",
    "    \n",
    "    `Xtr_noise = sigma_noise * np.random.randn(2*n, dim-2)`\n",
    "    \n",
    "    `Xts_noise = sigma_noise * np.random.randn(2*n, dim-2)`\n",
    "    \n",
    "    To compose the final data matrix, execute:\n",
    "    \n",
    "    `Xtr = np.concatenate((Xtr, Xtr_noise), axis=1)`\n",
    "    \n",
    "    `Xts = np.concatenate((Xts, Xts_noise), axis=1)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Principal Component Analysis\n",
    "\n",
    "* **2.A** Compute the data principal components (see help(PCA))\n",
    "\n",
    "\n",
    "* **2.B** Plot the first two components of X_proj using the following line\n",
    "\n",
    "    `plt.scatter(X_proj[:,0], X_proj[:,1], s=30, c=np.squeeze(Ytr), alpha=0.5)`\n",
    "    \n",
    "    `plt.show()`\n",
    "    \n",
    "    \n",
    "    \n",
    "* **2.C** Try now with the first 3 components, by using\n",
    "\n",
    "    `fig = pyplot.figure()`\n",
    "    `ax = Axes3D(fig)`\n",
    "\n",
    "    `x = X_proj[:,0].real`\n",
    "    `y = X_proj[:,1].real`\n",
    "    `z = X_proj[:,2].real`\n",
    "\n",
    "    `ax.scatter(x, y, z, c=np.squeeze(Ytr), marker='o')`\n",
    "\n",
    "    `ax.set_xlabel('X Label')`\n",
    "    `ax.set_ylabel('Y Label')`\n",
    "    `ax.set_zlabel('Z Label')`\n",
    "\n",
    "    `plt.show()`\n",
    "    \n",
    "Reason on the meaning of the results you are obtaining     \n",
    "    \n",
    "    \n",
    "    \n",
    "* **2.D** Display the sqrt of the first 10 eigenvalues `print(np.sqrt(d[:10]))`. Plot the coefficients (eigenvalues) associated with the largest eigenvalue:\n",
    "\n",
    "    `plt.scatter(range(dim), abs(d))`\n",
    "    \n",
    "    `plt.show()`\n",
    "\n",
    "\n",
    "\n",
    "* **2.D** Repeat the above steps with datasets generated using different sigma_noise `(0, 0.01, 0.1, 0.5, 1, 1.5, 2)`.  To what extent data visualization by PCA is affected by the noise?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Variable selection\n",
    "\n",
    "* **3.A** Use the data generated in section 1. Standardize the data matrix, so that each column has mean 0 and standard deviation 1\n",
    "\n",
    "    `m = np.mean(Xtr, axis=0)`\n",
    "\n",
    "    `s = np.std(Xtr, axis=0)`\n",
    "\n",
    "    `Xtr = (Xtr - m) / s`\n",
    "\n",
    "    Do te same for `Xtr`, by using means and standard deviation computed on `Xtr`\n",
    "    \n",
    "\n",
    "* **3.B** Use the orthogonal matching pursuit algorithm (type 'help(OMatchingPursuit)')\n",
    "\n",
    "\n",
    "* **3.C** You may want to check the predicted labels on the training set\n",
    "\n",
    "    `Ypred = np.sign(Xts * w)`\n",
    "    \n",
    "    `err = calcErr(Yts, Ypred)`\n",
    "    \n",
    "   and plot the coefficients `w` with `scatter(range(dim), np.abs(w))`. How does the error change with the number of iterations of the method?\n",
    "\n",
    "\n",
    "* **3.D** Repeat the experiment but this time using a dataset where the first two dimensions are gaussians with `means=[[1,1],[-1,-1]]`.  Plot the coefficients `w` with `scatter(range(dim), np.abs(w))`, what difference do you observe?\n",
    "\n",
    "\n",
    "* **3.E** By using the method `holdoutCVOMP` find the best number of iterations with `intIter = range(dim)` (and, for instance, `perc=0.75 nrip = 20`). Plot the training and validation error with the following lines of code:\n",
    "\n",
    "    `it, Vm, Vs, Tm, Ts = holdoutCVOMP(Xtr, Ytr, perc, nrip, intIter)`\n",
    "\n",
    "    `plt.plot(intIter, Tm, 'r+')`\n",
    "\n",
    "    `plt.plot(intIter, Vm, 'b+')`\n",
    "\n",
    "    `plt.title('Cross validation results', fontsize=20, color='red')`\n",
    "\n",
    "    `plt.xlabel('Number of dimension', fontsize=12, color='red')`\n",
    "    \n",
    "    `plt.ylabel('error', fontsize=12, color='red')`\n",
    "\n",
    "    `plt.show()`\n",
    "    \n",
    "    What is the behavior of the training and the validation errors with respect to the number of iterations?\n",
    "    \n",
    "    \n",
    "* **3.F** Try to increase the number of relevant variables d = 3,5,.. (and the corresponding standard deviation of the Gaussians) and see how this change is reflected in the cross-validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
