{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLCC - Laboratory 1 - Local methods\n",
    "In this laboratory we will address the problem of data analysis with a reference to a classification problem. \n",
    "Follow the instructions below.\n",
    "\n",
    "Import all the functions from the file \"lab1ImpFunction.py\" by: <br>\n",
    "`from lab1ImpFunction import *`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Warm up - data generation\n",
    "\n",
    "Loot at the function `MixGauss` in the file `lab1ImpFunction.py`\n",
    "* **1.A** The function  `MixGauss(means, sigmas, n)` generates dataset  `[X,Y]` where the `X` is composed of mixed classes, each class being generated according to a Gaussian distribution with given mean and standard deviation. The points in the dataset `X` are enumerated from 1 to n, and `Y` represents the label of each point. \n",
    "Hint: if the command help MixGauss fails, this probably means that you have not set up correctly your current working directory' \n",
    "Have a look at the code or, for a quick help, type \"help MixGauss\" on the Matlab shell.\n",
    "\n",
    "* **1.B** Type on the Matlab shell the commands\n",
    "`[X, Y] = MixGauss([[0;0],[1;1]],[0.5,0.25],50)\n",
    "figure(1); title('dataset 1');\n",
    "scatter(X(:,1),X(:,2),50,Y,'filled') %type \"help scatter\" to see what the parameters mean\n",
    "title('dataset 1')`\n",
    "\n",
    "* **1.C** Now generate a more complex dataset following the instructions below. This dataset will be referred hereafter as training dataset. \n",
    "Call MixGauss with appropriate parameters and produce a dataset with four classes: the classes must live in the 2D space and be centered on the corners of the unit square `(0,0), (0,1) (1,1), (1,0)`, all with standard deviation 0.3. \n",
    "The number of points in the dataset is up to you. \n",
    "`[Xtr,C]=MixGauss(....)`\n",
    "Use the Matlab function \"scatter\" to plot the training dataset.\n",
    "Manipulate the data so to obtain a 2-class problem where data on opposite corners share the same class. \n",
    "If you produced the data following the centers order provided above, you may do: `Ytr = 2*mod(C,2)-1`\n",
    "\n",
    "* **1.D** Following the same procedure as above (section 1.C) generate a new set of data `[Xte,Yte]` with the same distribution, hereafter called test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core - kNN classifier\n",
    "\n",
    "The k-Nearest Neighbors algorithm (kNN) assigns to a test point the most frequent label among its k closest points/examples in the training set. \n",
    "\n",
    "* **2.A** Have a look at the code of function kNNClassify (for a quick reference type \"help kNNClassify\" on the Matlab command prompt)\n",
    "\n",
    "* **2.B** Use kNNClassify on the previously generated 2-class data from section 1.D. Pick a \"reasonable\" k. Below we propose three ways of evaluating the quality of the prediction made by the kNN method. Try them and see the influence of the parameter k \n",
    "\n",
    "* **2.C1** [Evaluating the prediction] Plot the test data Xte twice. Once with its true labels Yte, and once with the predicted labels Ypred.  A possible way is:\n",
    "`figure;\n",
    "scatter(Xte(:,1),Xte(:,2),50,Yte,'filled'); %plot test points (filled circles) associating a different color to each \"true\" label\n",
    "hold on\n",
    "scatter(Xte(:,1),Xte(:,2),70,Ypred,'o'); % plot test points (empty circles) associating a different color to each estimated label`\n",
    "\n",
    "* **2.C2** [Evaluating the prediction] To compute the classification error percentage compare the estimated outputs with those previously  generated:\n",
    "`sum(Ypred ~= Yte) ./ size(Yte, 1)`\n",
    "\n",
    "* **2.C3** [Evaluating the prediction] To visualize the separating function, use the routine separatingFkNN. You may use help separatingFkNN in the command prompt or look directly at the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parameter selection - What is a good value for k?\n",
    "\n",
    "So far we considered an arbitrary k. We now want to inroduce different approaches for selecting it. \n",
    "\n",
    "* **3.A** Perform a hold out cross validation procedure on the available training data. \n",
    "You may want to use the function `holdoutCVkNN` available on the zip file (here again, type \"help holdoutCVkNN\" on the Matlab command prompt, you will find there a useful example of use).\n",
    "Plot the  training  and validation errors for the different values ok k. \n",
    "\n",
    "* **3.B** Add noise to the data by randomly flipping the labels on the training set, and call it Ytr noisy. You can use the function flipLabels to do that. How does the validation error behave now with respect to k ? \n",
    "Note: Keep track of the best k , and the corresponding validation error. You will need it in 3.D. \n",
    "\n",
    "* **3.C** What happens with different values of p (percentage of points held out) and rep (number of repetitions of the experiment)?\n",
    "\n",
    "* **3.D** For now we have been using the training set to obtain a classifier. Now we want to evaluate its performance by applying it to an independent test set.\n",
    "Consider the test dataset `[Xte,Yte]` generated in point 1.E. Add some noise to the dataset by randomly flipping some labels from `Yte`. You can use the function flipLabels to create `[Xte,Yte_noisy]`.\n",
    "Take the best `k` you obtained by hold out cross validation in 3.B, and use it to get a prediction from `Xtr,Ytrnoisy,Xte`, as you did in part 2.\n",
    "Evaluate the prediction with respect to Yte noisy (as you did in 2.C2), and compare it to the validation error you had in 3.B.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
