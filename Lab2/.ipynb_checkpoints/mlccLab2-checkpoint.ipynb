{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLCC - Laboratory 2 - Regularization networks \n",
    "In this laboratory we will see how to use regularization networks for regression and classification. Follow the instructions below. Think hard before you call the instructors!\n",
    "\n",
    "Import all the functions from the file \"lab2ImpFunction.py\" by: <br>\n",
    "`from lab2ImpFunction import *` <br>\n",
    "Also import pyplot for plotting: <br>\n",
    "`import matplotlib.pyplot as plt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Warm up - data generation\n",
    "\n",
    "* **1.A** Generate a 2-class training set by using the following code:\n",
    "\n",
    "    `Xtr, Ytr = MixGauss(means=[[0,0],[1,1]], sigmas=[0.5, 0.3], n=100)`\n",
    "\n",
    "    `Xte, Yte = MixGauss(means=[[0,0],[1,1]], sigmas=[0.5, 0.3], n=100)`\n",
    "\n",
    "    `Ytr = 2*np.mod(Ytr, 2)-1`\n",
    "\n",
    "    `Yte = 2*np.mod(Yte, 2)-1`\n",
    "    \n",
    "    \n",
    "* **1.B** Have a look at the code of functions regularizedKernLSTrain and regularizedKernLSTest . Use the former to generate a decision function.\n",
    "\n",
    "     `c = regularizedKernLSTrain(Xtr, Ytr, 'gaussian', sigma, lam)`\n",
    "     \n",
    "\n",
    "* **1.C** Check how the separating function changes with respect to `lam` and `sigma`. Use the Gaussian kernel `(kernel='gaussian')` and try some regularization parameters, with sigma in `[0.1, 10]` and lambda in `[1e-10, 10]`. To visualize the separating function (and thus get a more general view of what areas are associated with each class) you may use the routine `separatingFKernRLS` (type `help(separatingFKernRLS)` on a Jupyter Notebook cell, if you still have doubts on how to use it, have a look at the code). \n",
    "\n",
    "\n",
    "* **1.D** Perform the same experiment by using flipped labels (`Ytrn = flipLabels(Ytr, p)`) with `p=5` and `p=10`. Check how the separating function changes with respect to lambda.\n",
    "\n",
    "\n",
    "* **1.E** Load the Two moons dataset by using the command `Xtr, Ytr, Xte, Yte = two_moons(npoints, pflipped)` where npoints is the number of points in the dataset (between 1 and 100) and pflip is the percentage of flipped labels. Then visualize the training and the test set by the following lines.\n",
    "\n",
    "    `Xtr, Ytr, Xte, Yte = two_moons(100, 0)`\n",
    "    \n",
    "    `plt.figure(1)`\n",
    "    \n",
    "    `plt.scatter(Xtr[:,0], Xtr[:,1], s=50, c=Ytr)`\n",
    "\n",
    "    `plt.figure(2)`\n",
    "\n",
    "    `plt.scatter(Xte[:,0], Xte[:,1], s=50, c=Yte)`\n",
    "    \n",
    "    `plt.show()`\n",
    "    \n",
    "    \n",
    "* **1.F** Perform the exercises 1.C and 1.D on this dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Parameter Selection\n",
    "\n",
    "   * **2.A** By using the dataset in 1.E with 100 points and 5% flipped labels, select the suitable lambda, by using `holdoutCVKernRLS` (see `help(holdoutCVKernRLS)` for more informations), and the sequence\n",
    "   \n",
    "   `intKerPar = [0.5]`\n",
    "\n",
    "    `intLambda = [5, 2, 1, 0.7, 0.5, 0.3, 0.2, 0.1, 0.05, 0.02, 0.01, 0.005, 0.002, 0.001, 0.0005, 0.0002, 0.0001,0.00001,0.000001]`\n",
    "\n",
    "    `nrip = 5`\n",
    "\n",
    "    `perc = 50`\n",
    "    \n",
    "    Then plot the validation error and the test error with respect to the choice of lambda by the following code. (The x-axis has a logarithmic scale)\n",
    "    \n",
    "    `training_plot = plt.semilogx(lam_list, tm, 'r')`\n",
    "\n",
    "    `validation_plot = plt.semilogx(lam_list, vm, 'b')`\n",
    " \n",
    "    `plt.legend([mpatches.Patch(color='red'), mpatches.Patch(color='blue')], ['Training error', 'Validation error'])`\n",
    "\n",
    "    `plt.grid()`\n",
    "\n",
    "    `plt.show()`\n",
    "    \n",
    "    \n",
    "   * **2.B** Perform the same experiment for different fraction of flipped labels (0.0, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5). Check how the training and validation error change with different `p`.\n",
    "\n",
    "    \n",
    "   \n",
    "    \n",
    "   * **2.C** Now select the suitable sigma, by using `holdoutCVKernRLS`, on the following collection, as in exercise 2.A. Then plot the validation and test error.\n",
    "    \n",
    "    `kerpar_list = [10, 7, 5, 4, 3, 2.5, 2.0, 1.5, 1.0, 0.7, 0.5, 0.3, 0.2, 0.1, 0.05, 0.03, 0.02, 0.01]`\n",
    "\n",
    "    `lam_list = [0.00001]`\n",
    "\n",
    "    `nrip = 5`\n",
    "\n",
    "    `perc = 50`\n",
    "           \n",
    "           \n",
    "    \n",
    "   * **2.D** \tPerform the same experiment for different fraction of flipped labels (0.0, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5). Check how the training and validation error change with different `p`.\n",
    "   \n",
    "   \n",
    "   \n",
    "   * **2.E** Now select the best lambda and sigma, by using `holdoutCVKernRLS`, on the following collection, as in exercise 2.A. \n",
    "   `kerpar_list = [10, 7, 5, 4, 3, 2.5, 2.0, 1.5, 1.0, 0.7, 0.5, 0.3, 0.2, 0.1, 0.05, 0.03, 0.02, 0.01]`\n",
    "\n",
    "    `lam_list = [5, 2, 1, 0.7, 0.5, 0.3, 0.2, 0.1, 0.05, 0.02, 0.01, 0.005, 0.002, 0.001, 0.0005, 0.0002, 0.0001,0.00001,0.000001]`\n",
    "\n",
    "    `nrip = 7`\n",
    "\n",
    "    `perc = 50`\n",
    "\n",
    "    Then plot the separating function computed with the best lambda and sigma you have found. (use separatingFKernRLS).\n",
    "    \n",
    "    \n",
    " * **2.F** Compute the best lambda and sigma, and plot the related separating functions with 0%, 5%, 10%, 20%, 30%, 40% of flipped labels. How do the parameters differ, and the curves?\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you have time\n",
    "\n",
    "\n",
    "* **3.A** Repeat the experiment in section 2, with less points (70, 50, 30, 20) and 5% of flipped labels.\n",
    "\n",
    "    How do the parameters vary with respect to the number of points?\n",
    "    \n",
    "\n",
    "* **3.B** Repeat the experiment in section 1 with the polynomial kernel (kernel = 'polynomial') and with parameters lambda in the interval [10, 0] and p, the exponent of the polynomial kernel, in {10,9,...,1}.\n",
    "\n",
    "\n",
    "* **3.C** Perform the Exercise 2.F with the polynomial kernel and the following range of parameters.\n",
    "\n",
    "    `kerpar_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]`\n",
    "\n",
    "    `lam_list = [5, 2, 1, 0.7, 0.5, 0.3, 0.2, 0.1, 0.05, 0.02, 0.01, 0.005, 0.002, 0.001, 0.0005, 0.0002, 0.0001,0.00001,0.000001]`\n",
    "\n",
    "    What is the best exponent for the polynomial kernel on this problem? Why?\n",
    "\n",
    "\n",
    "* **3.D** Analyze the eigenvalues of the Gram matrix for the polynomial kernel with different values of p (plot them by using semilogy). What happens with different p? Why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
